{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝모델 BERT에 사용할\\\n",
    "한국어 워드피스 분철하는거 연습하는 공간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['안녕', '하', '세요', '나', '는', '김', '찬란', '입니다', '방구', '뿡뿡'], ['이', '영화', '정말', '재미있', '어요']]\n",
      "['안녕 하 세요 나 는 김 찬란 입니다 방구 뿡뿡', '이 영화 정말 재미있 어요']\n"
     ]
    }
   ],
   "source": [
    "#정제된 텍스트를 메캅으로 먼저 토큰화한다. => 교착어인 한국어의 단점을 보완하는 방법\n",
    "from konlpy.tag import Mecab\n",
    "mecab=Mecab(dicpath=r'C:/mecab/mecab-ko-dic')\n",
    "clean_texts= ['안녕하세요 나는 김찬란입니다 방구 뿡뿡','이 영화 정말 재미있어요']\n",
    "#여기서 '나는'이 워드피스 전처리후에도 그대로 나오기때문에 분리해주어야하는데 \n",
    "#그게 메캅을 지금하는 이유이다. '나' 와 '는' 은 각각의 뜻이 있다.\n",
    "#kiwi라는 토크나이저를 사용하면 더정확한 분해를 얻을수 있다고 한다.\n",
    "\n",
    "tokenized_clean_texts =[ mecab.morphs(i) for i in clean_texts ]\n",
    "print(tokenized_clean_texts)\n",
    "\n",
    "#토큰화된걸 ' ' 공백한칸을 기준으로 다시 합쳐준다.\n",
    "rejoined_texts = [' '.join(i) for i in tokenized_clean_texts]\n",
    "print(rejoined_texts)\n",
    "labels =[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# BertTokenizer : 버트에 사용될 워드피스 토큰화를 수행한다.\n",
    "# bert-base-multilingual-cased : \n",
    "# base는 12개 인코더(encoder)층이 있는 버전\n",
    "# multilingual은 여러언어를 사전학습한 모델이라는 뜻이다.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# # 텍스트 토큰화 함수  # 코느화되면 어떻게 되는지 확인해본것\n",
    "# def wordpiece_tokenize(text):\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     return tokens\n",
    "\n",
    "# wordpiece_tokenized =[]\n",
    "# for i in rejoined_texts:\n",
    "#     wordpiece_tokenized.append(wordpiece_tokenize(i))\n",
    "# print(wordpiece_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': <tf.Tensor: shape=(2, 64), dtype=int32, numpy=\n",
       "  array([[ 101, 9521,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 101, 9638,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0]])>,\n",
       "  'attention_masks': <tf.Tensor: shape=(2, 64), dtype=int32, numpy=\n",
       "  array([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>},\n",
       " <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1])>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "max_length = 64\n",
    "def preprocess(texts, labels):\n",
    "    input_ids =[]\n",
    "    attention_masks = []\n",
    "    for i in rejoined_texts:\n",
    "        input_tokens = tokenizer.batch_encode_plus(\n",
    "            i,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"tf\",\n",
    "            add_special_tokens=True, \n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(input_tokens['input_ids'][0])\n",
    "        attention_masks.append(input_tokens['attention_mask'][0])\n",
    "    # 차원 변경\n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "    return {'input_ids':input_ids, 'attention_masks':attention_masks}, labels\n",
    "\n",
    "preprocess(rejoined_texts,labels)\n",
    "\n",
    "# print(tf.convert_to_tensor(input_ids))\n",
    "# print(input_ids)\n",
    "# # print(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 데이터 분할\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 전처리 적용\n",
    "train_data, train_labels = preprocess(train_texts, train_labels)\n",
    "test_data, test_labels = preprocess(test_texts, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#버트 분류모델 불러오기\n",
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "# 모델 생성\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=64)\n",
    "\n",
    "# 모델 평가\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "print(\"Test loss, accuracy:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
