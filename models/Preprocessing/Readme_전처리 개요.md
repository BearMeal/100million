## 전처리기 개요- 23.04.12
1. 메캅토큰화,불용어제거 => TF-IDF 벡터화 => 모델1의 DNN, 모델2의 SVM에 적용되었음
   
2. 메캅토큰화 => 시계열 데이터(시퀀스) => RNN계열(lstm, gru, attention) 모델에 모두 적용
   => 불용어제거하고안하고 비교해봐야함
   => 불용어가 문장 구조와 문맥 이해에 중요한 역할을 할 수도 있다.

3. 메캅토큰화 => word2vec계열(skipgram,cbow,glove,fasttext)등 사전학습모델을 통해 (이 프로젝트에서는fasttext를 사용하며 직접 학습시켜본다. ) 벡터화(시퀀스데이터)
=> RNN모델에 모두 적용해본다.
=>메캅 토큰화, 마디분절 비교해봐야함
=> 불용어제거하고안하고 비교해봐야함
=> 자모를 분해해서하려면 텍스트에서 먼저 분해하고 그 텍스트를 불러와 학습시킨다.

### 서브워드임베딩방식
- 워드피스 토크나이저
1. 허깅페이스의 워드피스 => BERT모델에 적용

### fasttext와 wordpiece tokenizer를 하는이유
- 모델2, 3에서는 다중클래스 분류 학습데이터이기 때문에 기존의 방식으로는 OOV(out-of-vocabory)문제로 성능이 매우 낮게 나옴을 확인할수있었다. 따라서 이를 보완할수있는 방식들을 사용해볼것
- ngram을 통해 oov문제해결에 접근하는 fasttext와 
- 원래 단어를 분할하여 서브워드를 학습하는 방식으로 oov문제해결에 접근하는 wordpiece 방법을 사용해 볼것이다. 