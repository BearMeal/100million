{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DNN과 다르게 RNN에서는 okt형태소 토큰화 후 워드임베딩을 통해 문장의 문맥을 파악하는 방식으로 학습 시킬것이다. \n",
        "일반적으로 불용어를 제거=> 노이즈를 줄여서 성능향상을 도모\n",
        "그러나 감성분류 5가지에선 불용어 제거하지않고 형태소를 분리하고 어절을 그대로 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FlJcCm9HtWrN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "train_file = tf.keras.utils.get_file('train.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\n",
        "train_text = open(train_file,'rb').read().decode(encoding='utf-8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DUl7FhWgqpD6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.DataFrame({\n",
        "    'feature':[row.split('\\t')[1] for row in train_text.split('\\n')[1:] if row.count('\\t')>0],\n",
        "    'target': [ int(row.split('\\t')[2]) for row in train_text.split('\\n')[1:] if row.count('\\t')>0]\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#데이터 크기 조절\n",
        "df_train= df_train[::4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37500"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#문장 추출\n",
        "texts= [ ]\n",
        "for i in df_train['feature']:\n",
        "    texts.append(i)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#문자열이 아닌게 있는지 확인\n",
        "for i in texts:\n",
        "    if type(i)!=str:\n",
        "        print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train 데이터의 입력(X)에 대한 정제(Cleaning)\n",
        "import re\n",
        "from soynlp.normalizer import repeat_normalize\n",
        "\n",
        "def clean_korean_text(text):\n",
        "    # 특수 문자 및 숫자 제거\n",
        "    text = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]', '', text)\n",
        "    # 반복되는 자음, 모음 제거 (e.g., 'ㅋㅋㅋ' -> 'ㅋ')\n",
        "    text = repeat_normalize(text, num_repeats=1)\n",
        "    # 띄어쓰기 정규화 (연속된 공백 문자를 하나의 공백 문자로 변환)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "clean_texts=[]\n",
        "for i in texts:\n",
        "    clean_texts.append(clean_korean_text(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37500"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(clean_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Mecab 토큰화\n",
        "from konlpy.tag import Mecab\n",
        "mecab=Mecab(dicpath=r'C:/mecab/mecab-ko-dic')\n",
        "\n",
        "tokenized_text = [ mecab.morphs(i) for i in clean_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37500"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18678\n",
            "18822\n"
          ]
        }
      ],
      "source": [
        "#데이터 불균형 확인\n",
        "print(df_train.iloc[:,1][df_train.iloc[:,1]==0].count())\n",
        "print(df_train.iloc[:,1][df_train.iloc[:,1]==1].count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#y값 추출, 데이터 차원 맞추기, 넘파이 배열로 변환\n",
        "y_train=(df_train.iloc[:,1].to_frame()).to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#tokenized_text의 빈도수 상위 만개 어절을 토크나이저에 fit시킴\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(tokenized_text)\n",
        "\n",
        "#단어 빈도수 시각화\n",
        "#불용어제거후 시각화해볼수도있음\n",
        "\n",
        "#시퀀스데이터로 변환 ==벡터화\n",
        "sequences = tokenizer.texts_to_sequences(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "#길이를 맞춰주는 패딩작업,\n",
        "x_train = pad_sequences(sequences, maxlen=40, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 35  80 943  41 245  22  40 646   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(37500, 40)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#시퀀스 길이확인\n",
        "print(x_train[0])\n",
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "220/220 [==============================] - 36s 125ms/step - loss: 0.5526 - accuracy: 0.6847 - val_loss: 0.5114 - val_accuracy: 0.8095\n",
            "Epoch 2/20\n",
            "220/220 [==============================] - 28s 128ms/step - loss: 0.3565 - accuracy: 0.8479 - val_loss: 0.4276 - val_accuracy: 0.8103\n",
            "Epoch 3/20\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.2741 - accuracy: 0.8886 - val_loss: 0.4691 - val_accuracy: 0.8135\n",
            "Epoch 4/20\n",
            "220/220 [==============================] - 25s 112ms/step - loss: 0.2187 - accuracy: 0.9108 - val_loss: 0.5508 - val_accuracy: 0.8158\n",
            "Epoch 5/20\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.1749 - accuracy: 0.9312 - val_loss: 0.6412 - val_accuracy: 0.8112\n",
            "Epoch 6/20\n",
            "220/220 [==============================] - 25s 113ms/step - loss: 0.1454 - accuracy: 0.9451 - val_loss: 0.7282 - val_accuracy: 0.7977\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1e26a339db0>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# LSTM,gru\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding,BatchNormalization,Dropout\n",
        "from tensorflow.keras.layers import GRU, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding( 10000, 300, input_length=40 ),\n",
        "    LSTM(32,return_sequences=True),\n",
        "    GRU(32),\n",
        "    Dense(16, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 모델 컴파일\n",
        "optimizer= Adam(learning_rate =0.002)\n",
        "model.compile(optimizer=optimizer , loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#콜백정의\n",
        "ES =EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\n",
        "\n",
        "#모델학습\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=128, validation_split=0.25, callbacks=[ES])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "#테스트문장 입력\n",
        "test = ['회식끝나고 집가는중','개꿀잼이네','진짜 너무 별로다.','ㅋㅋㅋㅋㅋㅋㅋㅋ']\n",
        "\n",
        "#정제\n",
        "clean_test_texts=[]\n",
        "for i in test:\n",
        "    clean_test_texts.append(clean_korean_text(i))\n",
        "\n",
        "#mecab\n",
        "tokenized_test_text = [ mecab.morphs(i) for i in clean_test_texts]\n",
        "\n",
        "#벡터화\n",
        "test_sequences = tokenizer.texts_to_sequences(tokenized_test_text) \n",
        "x_test = pad_sequences(test_sequences,padding='post',maxlen=40) \n",
        "\n",
        "#예측\n",
        "prediction = model.predict(x_test)  \n",
        "\n",
        "for i in np.round(prediction):\n",
        "    print(int(i))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 카톡대화 입력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 카톡대화 불러와서 정제,(정규화코드)하는 함수\n",
        "import re\n",
        "\n",
        "def get_from_txt(txt):\n",
        "    data= open(txt,\"r\", encoding='utf-8').read().split('\\n')\n",
        "    sentences=[]\n",
        "    for line in data:\n",
        "        pattern = r'\\[(.*?)\\]\\s+\\[(.*?)\\]\\s+(.+)'\n",
        "        match = re.match(pattern, line)\n",
        "        if match:\n",
        "            name = match.group(1)  # 첫 번째 대괄호 안의 단어 추출\n",
        "            time = match.group(2)  # 두 번째 대괄호 안의 단어 추출\n",
        "            content = match.group(3)  # 대괄호 뒤의 내용 추출\n",
        "            # print(name, time, content)\n",
        "            temp=[name,time,content]\n",
        "            sentences.append(temp)    \n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "김하영\n"
          ]
        }
      ],
      "source": [
        "#닉네임 입력단 \n",
        "target_name = str(input())\n",
        "print(target_name)  #김하영 입력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 15ms/step\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문과 긍정문의 갯수: 77 98\n",
            "긍정과 부정의 비율:  1.2727272727272727\n"
          ]
        }
      ],
      "source": [
        "#입력된 이름의 '대화내역만' 담기\n",
        "received_texts= []\n",
        "for i in get_from_txt('sample.txt'): \n",
        "    if i[0] == target_name:\n",
        "        received_texts.append( i[2] )\n",
        "        \n",
        "# 이모티콘, 사진, 샵검색 제거 \n",
        "clean_received_texts1 = []\n",
        "for i in received_texts:\n",
        "    if '샵검색:' not in i: \n",
        "        if \"이모티콘\" not in i:\n",
        "            if '샵검색:' not in i:\n",
        "                clean_received_texts1.append(str(i))\n",
        "\n",
        "#=========== 이쯤에서 답장시간 계산기 구현   =======================\n",
        "          \n",
        "          \n",
        "#텍스트 정제  \n",
        "clean_received_texts2= [clean_korean_text(i) for i in clean_received_texts1]\n",
        "\n",
        "\n",
        "#문자열이 아닌게 있는지 확인\n",
        "for i in clean_received_texts2:\n",
        "    if type(i)!=str:\n",
        "        print(i)\n",
        "        \n",
        "# train 데이터 입력값(X)을 정제(Cleaning)\n",
        "import re\n",
        "from soynlp.normalizer import repeat_normalize\n",
        "\n",
        "def clean_korean_text(text):\n",
        "    # 특수 문자 및 숫자 제거\n",
        "    text = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]', '', text)\n",
        "    # 반복되는 자음, 모음 제거 (e.g., 'ㅋㅋㅋ' -> 'ㅋ')\n",
        "    text = repeat_normalize(text, num_repeats=1)\n",
        "    # 띄어쓰기 정규화 (연속된 공백 문자를 하나의 공백 문자로 변환)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "clean_test_texts=[]\n",
        "for i in clean_received_texts2:\n",
        "    clean_test_texts.append(clean_korean_text(i))\n",
        "\n",
        "\n",
        "#mecab으로 토큰화\n",
        "tokenized_clean_test_texts= [mecab.morphs(i) for i in clean_test_texts ]\n",
        "\n",
        "\n",
        "#시퀀스데이터로 변환\n",
        "test_sequences = tokenizer.texts_to_sequences(tokenized_clean_test_texts) \n",
        "paded_test_sequences = pad_sequences(test_sequences,padding='post',maxlen=40) \n",
        "prediction = model.predict(paded_test_sequences)  \n",
        "\n",
        "\n",
        "cnt0=0;cnt1=0\n",
        "\n",
        "for i in prediction:\n",
        "    if float(i)>=0.5:\n",
        "        print('긍정문입니다.')\n",
        "        cnt1+=1\n",
        "    elif float(i)<0.5:\n",
        "        print('부정문입니다.')\n",
        "        cnt0+=1\n",
        "        \n",
        "#부정과 긍정문의 갯수\n",
        "print('부정문과 긍정문의 갯수:',cnt0,cnt1)\n",
        "\n",
        "#긍정과 부정의 비율( 긍정문의 수 / 부정문의 수)\n",
        "print('긍정과 부정의 비율: ',cnt1/cnt0)\n",
        "#숫자가 1이상이고 높을수록 긍정적이다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "21ad9198a94b30f2d712b1b2e619acd2a12d5c96930278f82268986495e1541c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
