{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DNN과 다르게 RNN에서는 okt형태소 토큰화 후 워드임베딩을 통해 문장의 문맥을 파악하는 방식으로 학습 시킬것이다. \n",
        "일반적으로 불용어를 제거=> 노이즈를 줄여서 성능향상을 도모\n",
        "그러나 감성분류 5가지에선 불용어 제거하지않고 형태소를 분리하고 어절을 그대로 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FlJcCm9HtWrN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "train_file = tf.keras.utils.get_file('train.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\n",
        "train_text = open(train_file,'rb').read().decode(encoding='utf-8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DUl7FhWgqpD6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.DataFrame({\n",
        "    'feature':[row.split('\\t')[1] for row in train_text.split('\\n')[1:] if row.count('\\t')>0],\n",
        "    'target': [ int(row.split('\\t')[2]) for row in train_text.split('\\n')[1:] if row.count('\\t')>0]\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#데이터 크기 조절\n",
        "df_train= df_train[::4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37500"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#문장 추출\n",
        "texts= [ ]\n",
        "for i in df_train['feature']:\n",
        "    texts.append(i)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#문자열이 아닌게 있는지 확인\n",
        "for i in texts:\n",
        "    if type(i)!=str:\n",
        "        print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train 데이터의 입력(X)에 대한 정제(Cleaning)\n",
        "import re\n",
        "from soynlp.normalizer import repeat_normalize\n",
        "\n",
        "def clean_korean_text(text):\n",
        "    # 특수 문자 및 숫자 제거\n",
        "    text = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]', '', text)\n",
        "    # 반복되는 자음, 모음 제거 (e.g., 'ㅋㅋㅋ' -> 'ㅋ')\n",
        "    text = repeat_normalize(text, num_repeats=1)\n",
        "    # 띄어쓰기 정규화 (연속된 공백 문자를 하나의 공백 문자로 변환)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "clean_texts=[]\n",
        "for i in texts:\n",
        "    clean_texts.append(clean_korean_text(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37500"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(clean_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Mecab 토큰화\n",
        "from konlpy.tag import Mecab\n",
        "mecab=Mecab(dicpath=r'C:/mecab/mecab-ko-dic')\n",
        "\n",
        "tokenized_text = [ mecab.morphs(i) for i in clean_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37500"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18678\n",
            "18822\n"
          ]
        }
      ],
      "source": [
        "#데이터 불균형 확인\n",
        "print(df_train.iloc[:,1][df_train.iloc[:,1]==0].count())\n",
        "print(df_train.iloc[:,1][df_train.iloc[:,1]==1].count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#y값 추출, 데이터 차원 맞추기, 넘파이 배열로 변환\n",
        "y_train=(df_train.iloc[:,1].to_frame()).to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#tokenized_text의 빈도수 상위 만개 어절을 토크나이저에 fit시킴\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(tokenized_text)\n",
        "\n",
        "#단어 빈도수 시각화\n",
        "#불용어제거후 시각화해볼수도있음\n",
        "\n",
        "#시퀀스데이터로 변환 ==벡터화\n",
        "sequences = tokenizer.texts_to_sequences(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "#길이를 맞춰주는 패딩작업,\n",
        "x_train = pad_sequences(sequences, maxlen=40, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 35  80 943  41 245  22  40 646   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(37500, 40)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#시퀀스 길이확인\n",
        "print(x_train[0])\n",
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Cannot iterate over a Tensor with unknown first dimension.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\uk246\\Desktop\\일억조 프로젝트\\models\\모델1 감성(긍정부정)분류\\model_1 Transfomer.ipynb 셀 16\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/models/%EB%AA%A8%EB%8D%B81%20%EA%B0%90%EC%84%B1%28%EA%B8%8D%EC%A0%95%EB%B6%80%EC%A0%95%29%EB%B6%84%EB%A5%98/model_1%20Transfomer.ipynb#X21sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m x \u001b[39m=\u001b[39m Embedding(\u001b[39m10000\u001b[39m, \u001b[39m300\u001b[39m, input_length\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m)(input_layer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/models/%EB%AA%A8%EB%8D%B81%20%EA%B0%90%EC%84%B1%28%EA%B8%8D%EC%A0%95%EB%B6%80%EC%A0%95%29%EB%B6%84%EB%A5%98/model_1%20Transfomer.ipynb#X21sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m x \u001b[39m=\u001b[39m PositionalEncoding(\u001b[39m40\u001b[39m, \u001b[39m300\u001b[39m)(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/models/%EB%AA%A8%EB%8D%B81%20%EA%B0%90%EC%84%B1%28%EA%B8%8D%EC%A0%95%EB%B6%80%EC%A0%95%29%EB%B6%84%EB%A5%98/model_1%20Transfomer.ipynb#X21sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m x \u001b[39m=\u001b[39m transformer_encoder(x, d_model\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, num_heads\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m, ff_units\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/models/%EB%AA%A8%EB%8D%B81%20%EA%B0%90%EC%84%B1%28%EA%B8%8D%EC%A0%95%EB%B6%80%EC%A0%95%29%EB%B6%84%EB%A5%98/model_1%20Transfomer.ipynb#X21sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m x \u001b[39m=\u001b[39m Dense(\u001b[39m16\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m)(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/models/%EB%AA%A8%EB%8D%B81%20%EA%B0%90%EC%84%B1%28%EA%B8%8D%EC%A0%95%EB%B6%80%EC%A0%95%29%EB%B6%84%EB%A5%98/model_1%20Transfomer.ipynb#X21sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m x \u001b[39m=\u001b[39m BatchNormalization()(x)\n",
            "\u001b[1;32mc:\\Users\\uk246\\Desktop\\일억조 프로젝트\\models\\모델1 감성(긍정부정)분류\\model_1 Transfomer.ipynb 셀 16\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/models/%EB%AA%A8%EB%8D%B81%20%EA%B0%90%EC%84%B1%28%EA%B8%8D%EC%A0%95%EB%B6%80%EC%A0%95%29%EB%B6%84%EB%A5%98/model_1%20Transfomer.ipynb#X21sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransformer_encoder\u001b[39m(input_seq, d_model, num_heads, ff_units):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/models/%EB%AA%A8%EB%8D%B81%20%EA%B0%90%EC%84%B1%28%EA%B8%8D%EC%A0%95%EB%B6%80%EC%A0%95%29%EB%B6%84%EB%A5%98/model_1%20Transfomer.ipynb#X21sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     attn_output, _ \u001b[39m=\u001b[39m MultiHeadAttention(num_heads\u001b[39m=\u001b[39mnum_heads, key_dim\u001b[39m=\u001b[39md_model)(input_seq, input_seq)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/models/%EB%AA%A8%EB%8D%B81%20%EA%B0%90%EC%84%B1%28%EA%B8%8D%EC%A0%95%EB%B6%80%EC%A0%95%29%EB%B6%84%EB%A5%98/model_1%20Transfomer.ipynb#X21sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     attn_output \u001b[39m=\u001b[39m Dropout(\u001b[39m0.1\u001b[39m)(attn_output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/models/%EB%AA%A8%EB%8D%B81%20%EA%B0%90%EC%84%B1%28%EA%B8%8D%EC%A0%95%EB%B6%80%EC%A0%95%29%EB%B6%84%EB%A5%98/model_1%20Transfomer.ipynb#X21sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     out1 \u001b[39m=\u001b[39m LayerNormalization(epsilon\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m)(input_seq \u001b[39m+\u001b[39m attn_output)\n",
            "File \u001b[1;32mc:\\Users\\uk246\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\keras_tensor.py:408\u001b[0m, in \u001b[0;36mKerasTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot iterate over a scalar.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    407\u001b[0m \u001b[39mif\u001b[39;00m shape[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 408\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    409\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot iterate over a Tensor with unknown first dimension.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m     )\n\u001b[0;32m    411\u001b[0m \u001b[39mreturn\u001b[39;00m _KerasTensorIterator(\u001b[39mself\u001b[39m, shape[\u001b[39m0\u001b[39m])\n",
            "\u001b[1;31mTypeError\u001b[0m: Cannot iterate over a Tensor with unknown first dimension."
          ]
        }
      ],
      "source": [
        "#Transfomer 일단만들어\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding,BatchNormalization,Dropout\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add,Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 사용자 정의 Positional Encoding 레이어\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "            d_model=d_model\n",
        "        )\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "\n",
        "# Transformer Encoder 레이어\n",
        "def transformer_encoder(input_seq, d_model, num_heads, ff_units):\n",
        "    attn_output, _ = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(input_seq, input_seq)\n",
        "    attn_output = Dropout(0.1)(attn_output)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(input_seq + attn_output)\n",
        "\n",
        "    ff_output = Dense(ff_units, activation='relu')(out1)\n",
        "    ff_output = Dense(d_model)(ff_output)\n",
        "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ff_output)\n",
        "\n",
        "    return Flatten()(out2)\n",
        "\n",
        "# 입력 형태 정의\n",
        "input_layer = Input(shape=(40,))\n",
        "\n",
        "# Functional API를 사용하여 모델 구축\n",
        "x = Embedding(10000, 300, input_length=40)(input_layer)\n",
        "x = PositionalEncoding(40, 300)(x)\n",
        "x = transformer_encoder(x, d_model=300, num_heads=6, ff_units=1024)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output_layer = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# 모델 컴파일\n",
        "optimizer= Adam(learning_rate =0.002)\n",
        "model.compile(optimizer=optimizer , loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#콜백정의\n",
        "ES =EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True)\n",
        "\n",
        "#모델학습\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=64, validation_split=0.25, callbacks=[ES])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 39ms/step\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "#테스트문장 입력\n",
        "test = ['회식끝나고 집가는중','개꿀잼이네','진짜 너무 별로다.','ㅋㅋㅋㅋㅋㅋㅋㅋ']\n",
        "\n",
        "#정제\n",
        "clean_test_texts=[]\n",
        "for i in test:\n",
        "    clean_test_texts.append(clean_korean_text(i))\n",
        "\n",
        "#mecab\n",
        "tokenized_test_text = [ mecab.morphs(i) for i in clean_test_texts]\n",
        "\n",
        "#벡터화\n",
        "test_sequences = tokenizer.texts_to_sequences(tokenized_test_text) \n",
        "x_test = pad_sequences(test_sequences,padding='post',maxlen=40) \n",
        "\n",
        "#예측\n",
        "prediction = model.predict(x_test)  \n",
        "\n",
        "for i in np.round(prediction):\n",
        "    print(int(i))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 카톡대화 입력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 카톡대화 불러와서 정제,(정규화코드)하는 함수\n",
        "import re\n",
        "\n",
        "def get_from_txt(txt):\n",
        "    data= open(txt,\"r\", encoding='utf-8').read().split('\\n')\n",
        "    sentences=[]\n",
        "    for line in data:\n",
        "        pattern = r'\\[(.*?)\\]\\s+\\[(.*?)\\]\\s+(.+)'\n",
        "        match = re.match(pattern, line)\n",
        "        if match:\n",
        "            name = match.group(1)  # 첫 번째 대괄호 안의 단어 추출\n",
        "            time = match.group(2)  # 두 번째 대괄호 안의 단어 추출\n",
        "            content = match.group(3)  # 대괄호 뒤의 내용 추출\n",
        "            # print(name, time, content)\n",
        "            temp=[name,time,content]\n",
        "            sentences.append(temp)    \n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "김하영\n"
          ]
        }
      ],
      "source": [
        "#닉네임 입력단 \n",
        "target_name = str(input())\n",
        "print(target_name)  #김하영 입력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 15ms/step\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "긍정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문입니다.\n",
            "부정문과 긍정문의 갯수: 132 43\n",
            "긍정과 부정의 비율:  0.32575757575757575\n"
          ]
        }
      ],
      "source": [
        "#입력된 이름의 '대화내역만' 담기\n",
        "received_texts= []\n",
        "for i in get_from_txt('sample.txt'): \n",
        "    if i[0] == target_name:\n",
        "        received_texts.append( i[2] )\n",
        "        \n",
        "# 이모티콘, 사진, 샵검색 제거 \n",
        "clean_received_texts1 = []\n",
        "for i in received_texts:\n",
        "    if '샵검색:' not in i: \n",
        "        if \"이모티콘\" not in i:\n",
        "            if '샵검색:' not in i:\n",
        "                clean_received_texts1.append(str(i))\n",
        "\n",
        "#=========== 이쯤에서 답장시간 계산기 구현   =======================\n",
        "          \n",
        "          \n",
        "#텍스트 정제  \n",
        "clean_received_texts2= [clean_korean_text(i) for i in clean_received_texts1]\n",
        "\n",
        "\n",
        "#문자열이 아닌게 있는지 확인\n",
        "for i in clean_received_texts2:\n",
        "    if type(i)!=str:\n",
        "        print(i)\n",
        "        \n",
        "# train 데이터 입력값(X)을 정제(Cleaning)\n",
        "import re\n",
        "from soynlp.normalizer import repeat_normalize\n",
        "\n",
        "def clean_korean_text(text):\n",
        "    # 특수 문자 및 숫자 제거\n",
        "    text = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]', '', text)\n",
        "    # 반복되는 자음, 모음 제거 (e.g., 'ㅋㅋㅋ' -> 'ㅋ')\n",
        "    text = repeat_normalize(text, num_repeats=1)\n",
        "    # 띄어쓰기 정규화 (연속된 공백 문자를 하나의 공백 문자로 변환)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "clean_test_texts=[]\n",
        "for i in clean_received_texts2:\n",
        "    clean_test_texts.append(clean_korean_text(i))\n",
        "\n",
        "\n",
        "#mecab으로 토큰화\n",
        "tokenized_clean_test_texts= [mecab.morphs(i) for i in clean_test_texts ]\n",
        "\n",
        "\n",
        "#시퀀스데이터로 변환\n",
        "test_sequences = tokenizer.texts_to_sequences(tokenized_clean_test_texts) \n",
        "paded_test_sequences = pad_sequences(test_sequences,padding='post',maxlen=40) \n",
        "prediction = model.predict(paded_test_sequences)  \n",
        "\n",
        "\n",
        "cnt0=0;cnt1=0\n",
        "\n",
        "for i in prediction:\n",
        "    if float(i)>=0.5:\n",
        "        print('긍정문입니다.')\n",
        "        cnt1+=1\n",
        "    elif float(i)<0.5:\n",
        "        print('부정문입니다.')\n",
        "        cnt0+=1\n",
        "        \n",
        "#부정과 긍정문의 갯수\n",
        "print('부정문과 긍정문의 갯수:',cnt0,cnt1)\n",
        "\n",
        "#긍정과 부정의 비율( 긍정문의 수 / 부정문의 수)\n",
        "print('긍정과 부정의 비율: ',cnt1/cnt0)\n",
        "#숫자가 1이상이고 높을수록 긍정적이다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "21ad9198a94b30f2d712b1b2e619acd2a12d5c96930278f82268986495e1541c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
