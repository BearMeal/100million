{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5iox9kcqpD1"
      },
      "source": [
        "메켑연습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glBvON29q6q7",
        "outputId": "808af61f-df45-4bfd-e53b-ab32fd5d3410"
      },
      "outputs": [],
      "source": [
        "# !pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FlJcCm9HtWrN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#1. Naver Sentiment Movie Corpus v1.0 다운로드\n",
        "train_file = tf.keras.utils.get_file('train.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\n",
        "# test_file = tf.keras.utils.get_file( 'test.txt',  'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')\n",
        "\n",
        "#파일을 이진모드로 읽어온다. 디코드는 utf8로한다.\n",
        "train_text = open(train_file,'rb').read().decode(encoding='utf-8')\n",
        "# test_text = open(test_file,'rb').read().decode(encoding='utf-8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DUl7FhWgqpD6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.DataFrame({\n",
        "    'feature':[row.split('\\t')[1] for row in train_text.split('\\n')[1:] if row.count('\\t')>0],\n",
        "    'target': [ int(row.split('\\t')[2]) for row in train_text.split('\\n')[1:] if row.count('\\t')>0]\n",
        "})\n",
        "\n",
        "#데이터 크기 지정\n",
        "df_train = df_train[::6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#문장 추출\n",
        "texts= [ ]\n",
        "for i in df_train['feature']:\n",
        "    texts.append(i)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#문자열이 아닌게 있는지 확인\n",
        "for i in texts:\n",
        "    if type(i)!=str:\n",
        "        print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train 데이터 입력값(X)을 정제(Cleaning)\n",
        "import re\n",
        "from soynlp.normalizer import repeat_normalize\n",
        "\n",
        "def clean_korean_text(text):\n",
        "    # 특수 문자 및 숫자 제거\n",
        "    text = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]', '', text)\n",
        "    # 반복되는 자음, 모음 제거 (e.g., 'ㅋㅋㅋ' -> 'ㅋ')\n",
        "    text = repeat_normalize(text, num_repeats=1)\n",
        "    # 띄어쓰기 정규화 (연속된 공백 문자를 하나의 공백 문자로 변환)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    #정제할것 추가. => 고도화\n",
        "\n",
        "    return text\n",
        "\n",
        "clean_texts=[]\n",
        "for i in texts:\n",
        "    clean_texts.append(clean_korean_text(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['아 더빙 진짜 짜증나네요 목소리',\n",
              " '원작의 긴장감을 제대로 살려내지못했다',\n",
              " '울면서 손들고 횡단보도 건널때 뛰쳐나올뻔 이범수 연기 드럽게못해',\n",
              " '이건 정말 깨알 캐스팅과 질퍽하지않은 산뜻한 내용구성이 잘 버무러진 깨알일드',\n",
              " '주제는 좋은데 중반부터 지루하다']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# len(clean_texts)\n",
        "clean_texts[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['가',\n",
              " '가까스로',\n",
              " '가령',\n",
              " '각',\n",
              " '각각',\n",
              " '각자',\n",
              " '각종',\n",
              " '갖고말하자면',\n",
              " '같다',\n",
              " '같이',\n",
              " '개의치않고',\n",
              " '거니와',\n",
              " '거바',\n",
              " '거의',\n",
              " '것',\n",
              " '것과 같이',\n",
              " '것들',\n",
              " '게다가',\n",
              " '게우다',\n",
              " '겨우',\n",
              " '견지에서',\n",
              " '결과에 이르다',\n",
              " '결국',\n",
              " '결론을 낼 수 있다',\n",
              " '겸사겸사',\n",
              " '고려하면',\n",
              " '고로',\n",
              " '곧',\n",
              " '공동으로',\n",
              " '과',\n",
              " '과연',\n",
              " '관계가 있다',\n",
              " '관계없이',\n",
              " '관련이 있다',\n",
              " '관하여',\n",
              " '관한',\n",
              " '관해서는',\n",
              " '구',\n",
              " '구체적으로',\n",
              " '구토하다',\n",
              " '그',\n",
              " '그들',\n",
              " '그때',\n",
              " '그래',\n",
              " '그래도',\n",
              " '그래서',\n",
              " '그러나',\n",
              " '그러니',\n",
              " '그러니까',\n",
              " '그러면',\n",
              " '그러므로',\n",
              " '그러한즉',\n",
              " '그런 까닭에',\n",
              " '그런데',\n",
              " '그런즉',\n",
              " '그럼',\n",
              " '그럼에도 불구하고',\n",
              " '그렇게 함으로써',\n",
              " '그렇지',\n",
              " '그렇지 않다면',\n",
              " '그렇지 않으면',\n",
              " '그렇지만',\n",
              " '그렇지않으면',\n",
              " '그리고',\n",
              " '그리하여',\n",
              " '그만이다',\n",
              " '그에 따르는',\n",
              " '그위에',\n",
              " '그저',\n",
              " '그중에서',\n",
              " '그치지 않다',\n",
              " '근거로',\n",
              " '근거하여',\n",
              " '기대여',\n",
              " '기점으로',\n",
              " '기준으로',\n",
              " '기타',\n",
              " '까닭으로',\n",
              " '까악',\n",
              " '까지',\n",
              " '까지 미치다',\n",
              " '까지도',\n",
              " '꽈당',\n",
              " '끙끙',\n",
              " '끼익',\n",
              " '나',\n",
              " '나머지는',\n",
              " '남들',\n",
              " '남짓',\n",
              " '너',\n",
              " '너희',\n",
              " '너희들',\n",
              " '네',\n",
              " '넷',\n",
              " '년',\n",
              " '논하지 않다',\n",
              " '놀라다',\n",
              " '누가 알겠는가',\n",
              " '누구',\n",
              " '다른',\n",
              " '다른 방면으로',\n",
              " '다만',\n",
              " '다섯',\n",
              " '다소',\n",
              " '다수',\n",
              " '다시 말하자면',\n",
              " '다시말하면',\n",
              " '다음',\n",
              " '다음에',\n",
              " '다음으로',\n",
              " '단지',\n",
              " '답다',\n",
              " '당신',\n",
              " '당장',\n",
              " '대로 하다',\n",
              " '대하면',\n",
              " '대하여',\n",
              " '대해 말하자면',\n",
              " '대해서',\n",
              " '댕그',\n",
              " '더구나',\n",
              " '더군다나',\n",
              " '더라도',\n",
              " '더불어',\n",
              " '더욱더',\n",
              " '더욱이는',\n",
              " '도달하다',\n",
              " '도착하다',\n",
              " '동시에',\n",
              " '동안',\n",
              " '된바에야',\n",
              " '된이상',\n",
              " '두번째로',\n",
              " '둘',\n",
              " '둥둥',\n",
              " '뒤따라',\n",
              " '뒤이어',\n",
              " '든간에',\n",
              " '들',\n",
              " '등',\n",
              " '등등',\n",
              " '딩동',\n",
              " '따라',\n",
              " '따라서',\n",
              " '따위',\n",
              " '따지지 않다',\n",
              " '딱',\n",
              " '때',\n",
              " '때가 되어',\n",
              " '때문에',\n",
              " '또',\n",
              " '또한',\n",
              " '뚝뚝',\n",
              " '라 해도',\n",
              " '령',\n",
              " '로',\n",
              " '로 인하여',\n",
              " '로부터',\n",
              " '로써',\n",
              " '륙',\n",
              " '를',\n",
              " '마음대로',\n",
              " '마저',\n",
              " '마저도',\n",
              " '마치',\n",
              " '막론하고',\n",
              " '만 못하다',\n",
              " '만약',\n",
              " '만약에',\n",
              " '만은 아니다',\n",
              " '만이 아니다',\n",
              " '만일',\n",
              " '만큼',\n",
              " '말하자면',\n",
              " '말할것도 없고',\n",
              " '매',\n",
              " '매번',\n",
              " '메쓰겁다',\n",
              " '몇',\n",
              " '모',\n",
              " '모두',\n",
              " '무렵',\n",
              " '무릎쓰고',\n",
              " '무슨',\n",
              " '무엇',\n",
              " '무엇때문에',\n",
              " '물론',\n",
              " '및',\n",
              " '바꾸어말하면',\n",
              " '바꾸어말하자면',\n",
              " '바꾸어서 말하면',\n",
              " '바꾸어서 한다면',\n",
              " '바꿔 말하면',\n",
              " '바로',\n",
              " '바와같이',\n",
              " '밖에 안된다',\n",
              " '반대로',\n",
              " '반대로 말하자면',\n",
              " '반드시',\n",
              " '버금',\n",
              " '보는데서',\n",
              " '보다더',\n",
              " '보드득',\n",
              " '본대로',\n",
              " '봐',\n",
              " '봐라',\n",
              " '부류의 사람들',\n",
              " '부터',\n",
              " '불구하고',\n",
              " '불문하고',\n",
              " '붕붕',\n",
              " '비걱거리다',\n",
              " '비교적',\n",
              " '비길수 없다',\n",
              " '비로소',\n",
              " '비록',\n",
              " '비슷하다',\n",
              " '비추어 보아',\n",
              " '비하면',\n",
              " '뿐만 아니라',\n",
              " '뿐만아니라',\n",
              " '뿐이다',\n",
              " '삐걱',\n",
              " '삐걱거리다',\n",
              " '사',\n",
              " '삼',\n",
              " '상대적으로 말하자면',\n",
              " '생각한대로',\n",
              " '설령',\n",
              " '설마',\n",
              " '설사',\n",
              " '셋',\n",
              " '소생',\n",
              " '소인',\n",
              " '솨',\n",
              " '쉿',\n",
              " '습니까',\n",
              " '습니다',\n",
              " '시각',\n",
              " '시간',\n",
              " '시작하여',\n",
              " '시초에',\n",
              " '시키다',\n",
              " '실로',\n",
              " '심지어',\n",
              " '아',\n",
              " '아니',\n",
              " '아니나다를가',\n",
              " '아니라면',\n",
              " '아니면',\n",
              " '아니었다면',\n",
              " '아래윗',\n",
              " '아무거나',\n",
              " '아무도',\n",
              " '아야',\n",
              " '아울러',\n",
              " '아이',\n",
              " '아이고',\n",
              " '아이구',\n",
              " '아이야',\n",
              " '아이쿠',\n",
              " '아하',\n",
              " '아홉',\n",
              " '안 그러면',\n",
              " '않기 위하여',\n",
              " '않기 위해서',\n",
              " '알 수 있다',\n",
              " '알았어',\n",
              " '앗',\n",
              " '앞에서',\n",
              " '앞의것',\n",
              " '야',\n",
              " '약간',\n",
              " '양자',\n",
              " '어',\n",
              " '어기여차',\n",
              " '어느',\n",
              " '어느 년도',\n",
              " '어느것',\n",
              " '어느곳',\n",
              " '어느때',\n",
              " '어느쪽',\n",
              " '어느해',\n",
              " '어디',\n",
              " '어때',\n",
              " '어떠한',\n",
              " '어떤',\n",
              " '어떤것',\n",
              " '어떤것들',\n",
              " '어떻게',\n",
              " '어떻해',\n",
              " '어이',\n",
              " '어째서',\n",
              " '어쨋든',\n",
              " '어쩔수 없다',\n",
              " '어찌',\n",
              " '어찌됏든',\n",
              " '어찌됏어',\n",
              " '어찌하든지',\n",
              " '어찌하여',\n",
              " '언제',\n",
              " '언젠가',\n",
              " '얼마',\n",
              " '얼마 안 되는 것',\n",
              " '얼마간',\n",
              " '얼마나',\n",
              " '얼마든지',\n",
              " '얼마만큼',\n",
              " '얼마큼',\n",
              " '엉엉',\n",
              " '에',\n",
              " '에 가서',\n",
              " '에 달려 있다',\n",
              " '에 대해',\n",
              " '에 있다',\n",
              " '에 한하다',\n",
              " '에게',\n",
              " '에서',\n",
              " '여',\n",
              " '여기',\n",
              " '여덟',\n",
              " '여러분',\n",
              " '여보시오',\n",
              " '여부',\n",
              " '여섯',\n",
              " '여전히',\n",
              " '여차',\n",
              " '연관되다',\n",
              " '연이서',\n",
              " '영',\n",
              " '영차',\n",
              " '옆사람',\n",
              " '예',\n",
              " '예를 들면',\n",
              " '예를 들자면',\n",
              " '예컨대',\n",
              " '예하면',\n",
              " '오',\n",
              " '오로지',\n",
              " '오르다',\n",
              " '오자마자',\n",
              " '오직',\n",
              " '오호',\n",
              " '오히려',\n",
              " '와',\n",
              " '와 같은 사람들',\n",
              " '와르르',\n",
              " '와아',\n",
              " '왜',\n",
              " '왜냐하면',\n",
              " '외에도',\n",
              " '요만큼',\n",
              " '요만한 것',\n",
              " '요만한걸',\n",
              " '요컨대',\n",
              " '우르르',\n",
              " '우리',\n",
              " '우리들',\n",
              " '우선',\n",
              " '우에 종합한것과같이',\n",
              " '운운',\n",
              " '월',\n",
              " '위에서 서술한바와같이',\n",
              " '위하여',\n",
              " '위해서',\n",
              " '윙윙',\n",
              " '육',\n",
              " '으로',\n",
              " '으로 인하여',\n",
              " '으로서',\n",
              " '으로써',\n",
              " '을',\n",
              " '응',\n",
              " '응당',\n",
              " '의',\n",
              " '의거하여',\n",
              " '의지하여',\n",
              " '의해',\n",
              " '의해되다',\n",
              " '의해서',\n",
              " '이',\n",
              " '이 되다',\n",
              " '이 때문에',\n",
              " '이 밖에',\n",
              " '이 외에',\n",
              " '이 정도의',\n",
              " '이것',\n",
              " '이곳',\n",
              " '이때',\n",
              " '이라면',\n",
              " '이래',\n",
              " '이러이러하다',\n",
              " '이러한',\n",
              " '이런',\n",
              " '이럴정도로',\n",
              " '이렇게 많은 것',\n",
              " '이렇게되면',\n",
              " '이렇게말하자면',\n",
              " '이렇구나',\n",
              " '이로 인하여',\n",
              " '이르기까지',\n",
              " '이리하여',\n",
              " '이만큼',\n",
              " '이번',\n",
              " '이봐',\n",
              " '이상',\n",
              " '이어서',\n",
              " '이었다',\n",
              " '이와 같다',\n",
              " '이와 같은',\n",
              " '이와 반대로',\n",
              " '이와같다면',\n",
              " '이외에도',\n",
              " '이용하여',\n",
              " '이유만으로',\n",
              " '이젠',\n",
              " '이지만',\n",
              " '이쪽',\n",
              " '이천구',\n",
              " '이천육',\n",
              " '이천칠',\n",
              " '이천팔',\n",
              " '인 듯하다',\n",
              " '인젠',\n",
              " '일',\n",
              " '일것이다',\n",
              " '일곱',\n",
              " '일단',\n",
              " '일때',\n",
              " '일반적으로',\n",
              " '일지라도',\n",
              " '임에 틀림없다',\n",
              " '입각하여',\n",
              " '입장에서',\n",
              " '잇따라',\n",
              " '있다',\n",
              " '자',\n",
              " '자기',\n",
              " '자기집',\n",
              " '자마자',\n",
              " '자신',\n",
              " '잠깐',\n",
              " '잠시',\n",
              " '저',\n",
              " '저것',\n",
              " '저것만큼',\n",
              " '저기',\n",
              " '저쪽',\n",
              " '저희',\n",
              " '전부',\n",
              " '전자',\n",
              " '전후',\n",
              " '점에서 보아',\n",
              " '정도에 이르다',\n",
              " '제',\n",
              " '제각기',\n",
              " '제외하고',\n",
              " '조금',\n",
              " '조차',\n",
              " '조차도',\n",
              " '졸졸',\n",
              " '좀',\n",
              " '좋아',\n",
              " '좍좍',\n",
              " '주룩주룩',\n",
              " '주저하지 않고',\n",
              " '줄은 몰랏다',\n",
              " '줄은모른다',\n",
              " '중에서',\n",
              " '중의하나',\n",
              " '즈음하여',\n",
              " '즉',\n",
              " '즉시',\n",
              " '지든지',\n",
              " '지만',\n",
              " '지말고',\n",
              " '진짜로',\n",
              " '쪽으로',\n",
              " '차라리',\n",
              " '참',\n",
              " '참나',\n",
              " '첫번째로',\n",
              " '쳇',\n",
              " '총적으로',\n",
              " '총적으로 말하면',\n",
              " '총적으로 보면',\n",
              " '칠',\n",
              " '콸콸',\n",
              " '쾅쾅',\n",
              " '쿵',\n",
              " '타다',\n",
              " '타인',\n",
              " '탕탕',\n",
              " '토하다',\n",
              " '통하여',\n",
              " '툭',\n",
              " '퉤',\n",
              " '틈타',\n",
              " '팍',\n",
              " '팔',\n",
              " '퍽',\n",
              " '펄렁',\n",
              " '하',\n",
              " '하게될것이다',\n",
              " '하게하다',\n",
              " '하겠는가',\n",
              " '하고 있다',\n",
              " '하고있었다',\n",
              " '하곤하였다',\n",
              " '하구나',\n",
              " '하기 때문에',\n",
              " '하기 위하여',\n",
              " '하기는한데',\n",
              " '하기만 하면',\n",
              " '하기보다는',\n",
              " '하기에',\n",
              " '하나',\n",
              " '하느니',\n",
              " '하는 김에',\n",
              " '하는 편이 낫다',\n",
              " '하는것도',\n",
              " '하는것만 못하다',\n",
              " '하는것이 낫다',\n",
              " '하는바',\n",
              " '하더라도',\n",
              " '하도다',\n",
              " '하도록시키다',\n",
              " '하도록하다',\n",
              " '하든지',\n",
              " '하려고하다',\n",
              " '하마터면',\n",
              " '하면 할수록',\n",
              " '하면된다',\n",
              " '하면서',\n",
              " '하물며',\n",
              " '하여금',\n",
              " '하여야',\n",
              " '하자마자',\n",
              " '하지 않는다면',\n",
              " '하지 않도록',\n",
              " '하지마',\n",
              " '하지마라',\n",
              " '하지만',\n",
              " '하하',\n",
              " '한 까닭에',\n",
              " '한 이유는',\n",
              " '한 후',\n",
              " '한다면',\n",
              " '한다면 몰라도',\n",
              " '한데',\n",
              " '한마디',\n",
              " '한적이있다',\n",
              " '한켠으로는',\n",
              " '한항목',\n",
              " '할 따름이다',\n",
              " '할 생각이다',\n",
              " '할 줄 안다',\n",
              " '할 지경이다',\n",
              " '할 힘이 있다',\n",
              " '할때',\n",
              " '할만하다',\n",
              " '할망정',\n",
              " '할뿐',\n",
              " '할수있다',\n",
              " '할수있어',\n",
              " '할줄알다',\n",
              " '할지라도',\n",
              " '할지언정',\n",
              " '함께',\n",
              " '해도된다',\n",
              " '해도좋다',\n",
              " '해봐요',\n",
              " '해서는 안된다',\n",
              " '해야한다',\n",
              " '해요',\n",
              " '했어요',\n",
              " '향하다',\n",
              " '향하여',\n",
              " '향해서',\n",
              " '허',\n",
              " '허걱',\n",
              " '허허',\n",
              " '헉',\n",
              " '헉헉',\n",
              " '헐떡헐떡',\n",
              " '형식으로 쓰여',\n",
              " '혹시',\n",
              " '혹은',\n",
              " '혼자',\n",
              " '훨씬',\n",
              " '휘익',\n",
              " '휴',\n",
              " '흐흐',\n",
              " '흥',\n",
              " '힘입어']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#불용어사전 가져와서 리스트로 만들기\n",
        "stop_words = open('./stopwords.txt','rb').read().decode(encoding='utf-8')\n",
        "\n",
        "#리스트 컴프리헨션\n",
        "stop_words = [ i for i in stop_words.split('\\n') ]\n",
        "stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class A:=> 클래스에 함수가 들어가면 == 메서드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb3u5TZqqpD7",
        "outputId": "bc835d99-5d1e-4fe3-e2fb-00b681fb8624"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\uk246\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "Exception",
          "evalue": "Install MeCab in order to use it: http://konlpy.org/en/latest/install/",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\uk246\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\konlpy\\tag\\_mecab.py:77\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[1;34m(self, dicpath)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagger \u001b[39m=\u001b[39m Tagger(\u001b[39m'\u001b[39m\u001b[39m-d \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m dicpath)\n\u001b[0;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagset \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mread_json(\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/data/tagset/mecab.json\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m utils\u001b[39m.\u001b[39minstallpath)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'Tagger' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\uk246\\Desktop\\일억조 프로젝트\\모델링\\EDA연습\\macab연습2.ipynb 셀 11\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/%EB%AA%A8%EB%8D%B8%EB%A7%81/EDA%EC%97%B0%EC%8A%B5/macab%EC%97%B0%EC%8A%B52.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#문장을 쪼개는 토큰화  +  불용어 제거(은는이가)  + 수치형으로 바꾸는 벡터화 동시에   \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/%EB%AA%A8%EB%8D%B8%EB%A7%81/EDA%EC%97%B0%EC%8A%B5/macab%EC%97%B0%EC%8A%B52.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer( tokenizer \u001b[39m=\u001b[39m mecab_tokenizer, stop_words \u001b[39m=\u001b[39m stop_words )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/%EB%AA%A8%EB%8D%B8%EB%A7%81/EDA%EC%97%B0%EC%8A%B5/macab%EC%97%B0%EC%8A%B52.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m X_train \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform( clean_texts )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/%EB%AA%A8%EB%8D%B8%EB%A7%81/EDA%EC%97%B0%EC%8A%B5/macab%EC%97%B0%EC%8A%B52.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# vectorizer 객체(=클래스)를 저장합니다.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/%EB%AA%A8%EB%8D%B8%EB%A7%81/EDA%EC%97%B0%EC%8A%B5/macab%EC%97%B0%EC%8A%B52.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmecab_test_vectorizer.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
            "File \u001b[1;32mc:\\Users\\uk246\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2121\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2116\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2117\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2118\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2119\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2120\u001b[0m )\n\u001b[1;32m-> 2121\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2123\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2124\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\uk246\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1377\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1369\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1370\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1374\u001b[0m             )\n\u001b[0;32m   1375\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1380\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\uk246\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1264\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1263\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1264\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1265\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1266\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
            "File \u001b[1;32mc:\\Users\\uk246\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m     doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     doc \u001b[39m=\u001b[39m tokenizer(doc)\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;32mc:\\Users\\uk246\\Desktop\\일억조 프로젝트\\모델링\\EDA연습\\macab연습2.ipynb 셀 11\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/%EB%AA%A8%EB%8D%B8%EB%A7%81/EDA%EC%97%B0%EC%8A%B5/macab%EC%97%B0%EC%8A%B52.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmecab_tokenizer\u001b[39m(text):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/%EB%AA%A8%EB%8D%B8%EB%A7%81/EDA%EC%97%B0%EC%8A%B5/macab%EC%97%B0%EC%8A%B52.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     mecab \u001b[39m=\u001b[39m Mecab()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/uk246/Desktop/%EC%9D%BC%EC%96%B5%EC%A1%B0%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/%EB%AA%A8%EB%8D%B8%EB%A7%81/EDA%EC%97%B0%EC%8A%B5/macab%EC%97%B0%EC%8A%B52.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m mecab\u001b[39m.\u001b[39mmorphs(text)\n",
            "File \u001b[1;32mc:\\Users\\uk246\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\konlpy\\tag\\_mecab.py:82\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[1;34m(self, dicpath)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThe MeCab dictionary does not exist at \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. Is the dictionary correctly installed?\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mYou can also try entering the dictionary path when initializing the Mecab class: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMecab(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m/some/dic/path\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m dicpath)\n\u001b[0;32m     81\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNameError\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInstall MeCab in order to use it: http://konlpy.org/en/latest/install/\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mException\u001b[0m: Install MeCab in order to use it: http://konlpy.org/en/latest/install/"
          ]
        }
      ],
      "source": [
        "#tfidf하면서 토큰화는 okt.morphs로 한다.=> 매켑으로해본다.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from konlpy.tag import Mecab\n",
        "import pickle\n",
        "\n",
        "#vectorizer객체를 저장하기위해 사용자지정 함수생성\n",
        "#메켑 함수 선언\n",
        "def mecab_tokenizer(text):\n",
        "    mecab = Mecab()\n",
        "    return mecab.morphs(text)\n",
        "\n",
        "#문장을 쪼개는 토큰화  +  불용어 제거(은는이가)  + 수치형으로 바꾸는 벡터화 동시에   \n",
        "vectorizer = TfidfVectorizer( tokenizer = mecab_tokenizer, stop_words = stop_words )\n",
        "\n",
        "X_train = vectorizer.fit_transform( clean_texts )\n",
        "\n",
        "# vectorizer 객체(=클래스)를 저장합니다.\n",
        "with open('mecab_test_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "    \n",
        "\n",
        "#만약 테스트문장 똑같은 벡터화과정을 거쳐야한다.\n",
        "# vectorizer.transform( X_test )\n",
        "# y = df_train.iloc[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# 저장된 vectorizer 객체를 불러옵니다.\n",
        "with open('test_vectorizer.pkl', 'rb') as f:\n",
        "    loaded_test_vectorizer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "test_texts = ['이거 정말 잼따. ', '아니 무슨 이딴게 다있냐;;','ㅋ']\n",
        "\n",
        "#문자열이 아닌게 있는지 확인\n",
        "for i in test_texts:\n",
        "    if type(i)!=str:\n",
        "        print(i)\n",
        "        \n",
        "# train 데이터 입력값(X)을 정제(Cleaning)\n",
        "import re\n",
        "from soynlp.normalizer import repeat_normalize\n",
        "\n",
        "def clean_korean_text(text):\n",
        "    # 특수 문자 및 숫자 제거\n",
        "    text = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]', '', text)\n",
        "\n",
        "    # 반복되는 자음, 모음 제거 (e.g., 'ㅋㅋㅋ' -> 'ㅋ')\n",
        "    text = repeat_normalize(text, num_repeats=1)\n",
        "\n",
        "    # 띄어쓰기 정규화 (연속된 공백 문자를 하나의 공백 문자로 변환)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "clean_test_texts=[]\n",
        "for i in test_texts:\n",
        "    clean_test_texts.append(clean_korean_text(i))\n",
        "\n",
        "# 학습된 TF-IDF 벡터라이저로 테스트 데이터 변환\n",
        "X_test_sample = loaded_lambda_vectorizer.transform(clean_test_texts)\n",
        "X_test_sample_dense = X_test_sample.toarray()\n",
        "print(X_test_sample_dense)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUU3nDwiFMxz",
        "outputId": "975a9dab-bb12-4ac6-db51-3a357ed28ae2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000, 2)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#불균형데이터확인\n",
        "df_train[df_train.iloc[:,1]==0]  #18678 \n",
        "df_train[df_train.iloc[:,1]==1]  #18822\n",
        "df_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXsMQ5HmuoR1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "35800"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "X_train_dense = np.expand_dims( X.toarray(), axis=-1 )\n",
        "# X_train_dense.shape\n",
        "# X.shape[1]\n",
        "# X.toarray().shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge7NOgY8uoKe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "350/350 [==============================] - 18s 46ms/step - loss: 0.6051 - accuracy: 0.6861 - val_loss: 0.5580 - val_accuracy: 0.8011\n",
            "Epoch 2/20\n",
            "350/350 [==============================] - 15s 41ms/step - loss: 0.3765 - accuracy: 0.8358 - val_loss: 0.4189 - val_accuracy: 0.8019\n",
            "Epoch 3/20\n",
            "350/350 [==============================] - 14s 40ms/step - loss: 0.2565 - accuracy: 0.8977 - val_loss: 0.4598 - val_accuracy: 0.8052\n",
            "Epoch 4/20\n",
            "350/350 [==============================] - 14s 40ms/step - loss: 0.1975 - accuracy: 0.9254 - val_loss: 0.5203 - val_accuracy: 0.7955\n",
            "Epoch 5/20\n",
            "350/350 [==============================] - 15s 42ms/step - loss: 0.1598 - accuracy: 0.9409 - val_loss: 0.5469 - val_accuracy: 0.8003\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x25b25af7a60>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#DNN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization , Input, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model = Sequential([\n",
        "    # input_layer,\n",
        "    Input(shape=(X_train_dense.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.6),\n",
        "    Dense(32, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.6),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 모델 컴파일 및 학습\n",
        "optimizer= Adam(learning_rate=0.002)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 콜백 정의\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
        "\n",
        "# model.fit(X.toarray(), y, epochs = 10, batch_size = 500, validation_split=0.2)  #0.8417\n",
        "model.fit(\n",
        "    X.toarray(), y, \n",
        "    epochs = 20, batch_size = 50, \n",
        "    validation_split=0.3,\n",
        "    callbacks=[early_stopping]\n",
        "    ) \n",
        "\n",
        "#Okt()토크나이저를 넣지 않았을 때\n",
        "# loss: 0.1659 - accuracy: 0.9387 - val_loss: 0.5841 - val_accuracy: 0.7403\n",
        "#okt() 토크나이저를 썼을 때\n",
        "# loss: 0.1598 - accuracy: 0.9409 - val_loss: 0.5469 - val_accuracy: 0.8003\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EClS3xeynli"
      },
      "outputs": [],
      "source": [
        "# model.save('3_31_DNN 튜닝모델.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl6oAcdct68I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n",
            "[[0.7537623 ]\n",
            " [0.02477952]\n",
            " [0.65656054]]\n",
            "예측 결과: [[1]\n",
            " [0]\n",
            " [1]]\n"
          ]
        }
      ],
      "source": [
        "test_texts = ['이거 정말 잼따. ', '아니 무슨 이딴게 다있냐;;','ㅋ']\n",
        "\n",
        "#문자열이 아닌게 있는지 확인\n",
        "for i in test_texts:\n",
        "    if type(i)!=str:\n",
        "        print(i)\n",
        "        \n",
        "# train 데이터 입력값(X)을 정제(Cleaning)\n",
        "import re\n",
        "from soynlp.normalizer import repeat_normalize\n",
        "\n",
        "def clean_korean_text(text):\n",
        "    # 특수 문자 및 숫자 제거\n",
        "    text = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]', '', text)\n",
        "\n",
        "    # 반복되는 자음, 모음 제거 (e.g., 'ㅋㅋㅋ' -> 'ㅋ')\n",
        "    text = repeat_normalize(text, num_repeats=1)\n",
        "\n",
        "    # 띄어쓰기 정규화 (연속된 공백 문자를 하나의 공백 문자로 변환)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "clean_test_texts=[]\n",
        "for i in test_texts:\n",
        "    clean_test_texts.append(clean_korean_text(i))\n",
        "\n",
        "# 학습된 TF-IDF 벡터라이저로 테스트 데이터 변환\n",
        "X_test_sample = vectorizer.transform(clean_test_texts)\n",
        "X_test_sample_dense = X_test_sample.toarray()\n",
        "\n",
        "# 모델을 사용하여 예측 수행\n",
        "predictions = model.predict(X_test_sample_dense)\n",
        "print(predictions)\n",
        "\n",
        "# 예측 결과 출력 (긍정: 1, 부정: 0)\n",
        "print(\"예측 결과:\", np.round( predictions ).astype(int))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "닉네임기준으로 \n",
        "카톡대화내용을 기준으로 테스트 문장을 여러개 불러와서 예측 후 그 값의 비율을 계산 출력해준다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 카톡대화 불러와서 정제,(정규화코드)하는 함수\n",
        "import re\n",
        "\n",
        "def get_from_txt(txt):\n",
        "    data= open(txt,\"r\", encoding='utf-8').read().split('\\n')\n",
        "    sentences=[]\n",
        "    for line in data:\n",
        "        pattern = r'\\[(.*?)\\]\\s+\\[(.*?)\\]\\s+(.+)'\n",
        "        match = re.match(pattern, line)\n",
        "        if match:\n",
        "            name = match.group(1)  # 첫 번째 대괄호 안의 단어 추출\n",
        "            time = match.group(2)  # 두 번째 대괄호 안의 단어 추출\n",
        "            content = match.group(3)  # 대괄호 뒤의 내용 추출\n",
        "            # print(name, time, content)\n",
        "            temp=[name,time,content]\n",
        "            sentences.append(temp)    \n",
        "    return sentences\n",
        "\n",
        "#함수확인\n",
        "get_from_txt('sample.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "김하영\n"
          ]
        }
      ],
      "source": [
        "#닉네임 입력단 \n",
        "target_name = str(input())\n",
        "print(target_name)  #김하영 입력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 5ms/step\n",
            "[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]\n",
            "부정문과 긍정문의 갯수: 87 88\n",
            "긍정과 부정의 비율:  1.0114942528735633\n"
          ]
        }
      ],
      "source": [
        "# 저장된 모델 불러오기\n",
        "# from tensorflow.keras.models import load_models\n",
        "# model= load_models('3_31_DNN 튜닝모델.h5')\n",
        "\n",
        "\n",
        "#입력된 이름의 '대화내역만' 담기\n",
        "received_texts= []\n",
        "for i in get_from_txt('sample.txt'): \n",
        "    if i[0] == target_name:\n",
        "        received_texts.append( i[2] )\n",
        "        \n",
        "# 이모티콘, 사진, 샵검색 제거 \n",
        "clean1_received_texts = []\n",
        "for i in received_texts:\n",
        "    if '샵검색:' not in i: \n",
        "        if \"이모티콘\" not in i:\n",
        "            if '샵검색:' not in i:\n",
        "                clean1_received_texts.append(str(i))\n",
        "\n",
        "#=========== 이쯤에서 답장시간 계산기 구현   =======================\n",
        "          \n",
        "          \n",
        "#텍스트 정제  \n",
        "clean2_received_texts= [clean_korean_text(i) for i in clean1_received_texts]\n",
        "\n",
        "# # 학습된 TF-IDF 벡터라이저로 테스트 데이터 변환\n",
        "X_received_texts = vectorizer.transform(clean2_received_texts)\n",
        "X_received_texts_dense = X_received_texts.toarray()\n",
        "\n",
        "\n",
        "# # 모델을 사용하여 예측 수행\n",
        "predictions = np.round(model.predict(X_received_texts_dense))\n",
        "#예측 결과 출력 (긍정: 1, 부정: 0)\n",
        "print(predictions.squeeze().tolist()) #차원을 축소하고 리스트객체로 변환\n",
        "\n",
        "cnt0=0;cnt1=0\n",
        "for i in predictions.squeeze().tolist():\n",
        "    if int(i)==0:\n",
        "        cnt0+=1\n",
        "    else:\n",
        "        cnt1+=1\n",
        "\n",
        "#부정과 긍정문의 갯수\n",
        "print('부정문과 긍정문의 갯수:',cnt0,cnt1)\n",
        "\n",
        "#긍정과 부정의 비율( 긍정문의 수 / 부정문의 수)\n",
        "print('긍정과 부정의 비율: ',cnt1/cnt0)\n",
        "#숫자가 1이상이고 높을수록 긍정적이다.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
