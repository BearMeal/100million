{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAlbertForSequenceClassification\n",
    "from transformers import BertTokenizerFast\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFAlbertForSequenceClassification.\n",
      "\n",
      "All the layers of TFAlbertForSequenceClassification were initialized from the model checkpoint at ./emo_finetuned_albert-kor-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 파인튜닝된 모델을 가중치와 함께 불러온다.\n",
    "loaded_model = TFAlbertForSequenceClassification.from_pretrained('./emo_finetuned_albert-kor-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizerFast'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizerFast'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "example_texts = ['넌 별로지만 좋기도해','너무 안좋다..',';;;; ...','대박이네','좋을지도?']\n",
    "\n",
    "# 토큰화진행 \n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"kykim/albert-kor-base\")\n",
    "#토큰화된 문장의길이는 임의로 64지정\n",
    "test_encodings = tokenizer(example_texts, truncation=True, padding='max_length', max_length=64,return_token_type_ids= False) \n",
    "\n",
    "# 새로운 데이터를 입력 데이터로 변환\n",
    "test_input_ids = np.array(test_encodings['input_ids'])\n",
    "test_attention_mask = np.array(test_encodings['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 39712, 8144, 2033, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['[CLS]', '좋을지', '##도', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[CLS] 좋을지도? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "#데이터확인\n",
    "n=4\n",
    "#수치형 데이터\n",
    "print(test_encodings['input_ids'][n])\n",
    "#토큰화된 텍스트\n",
    "print(tokenizer.convert_ids_to_tokens(test_encodings['input_ids'][n]))\n",
    "#원본텍스트\n",
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(test_encodings['input_ids'][n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 960ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.7469093,  3.6134062, -1.0537384, -1.2189223, -0.4793804],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = loaded_model.predict([test_input_ids, test_attention_mask])\n",
    "predicted[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.01220285 0.9552613  0.00897857 0.00761147 0.01594584], shape=(5,), dtype=float32)\n",
      "[0 1 0 0 0]\n",
      "tf.Tensor([9.9923503e-01 3.6839326e-04 1.4503566e-04 8.2970095e-05 1.6857049e-04], shape=(5,), dtype=float32)\n",
      "[1 0 0 0 0]\n",
      "tf.Tensor([2.0364339e-04 9.9921119e-01 2.8810793e-04 1.6516287e-04 1.3183978e-04], shape=(5,), dtype=float32)\n",
      "[0 1 0 0 0]\n",
      "tf.Tensor([0.00126993 0.0016664  0.00704778 0.62186646 0.36814937], shape=(5,), dtype=float32)\n",
      "[0 0 0 1 0]\n",
      "tf.Tensor([1.1093362e-04 1.5350366e-03 6.8980968e-03 9.9043989e-01 1.0160032e-03], shape=(5,), dtype=float32)\n",
      "[0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "for i in predicted[0]:\n",
    "    probabilities = tf.nn.softmax(i)\n",
    "    print(probabilities)\n",
    "    print(np.round(probabilities).astype(int))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
