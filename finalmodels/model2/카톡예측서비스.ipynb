{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 모델을 불러와서 카톡대화를 넣으면\\\n",
    "강한부정,부정,중성,긍정,강한긍정 [12, 23, 45, 65, 34]  형태로 반환해준다.\n",
    "위 리스트의 인덱스를호출하면 해당 점수의 문장 3개를 뽑아준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFAlbertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFAlbertForSequenceClassification.\n",
      "\n",
      "All the layers of TFAlbertForSequenceClassification were initialized from the model checkpoint at ./albert-kor-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 저장된 모델 불러오기\n",
    "loaded_model = TFAlbertForSequenceClassification.from_pretrained('./albert-kor-base')\n",
    "\n",
    "    \n",
    "# 피클 파일에서 BertTokenizerFast 객체 로드\n",
    "with open('./albert_tokenizer.pkl', \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카톡대화 불러와서 정제(정규화코드)하는 함수\n",
    "def get_from_txt(txt):\n",
    "    data= open(txt,\"r\", encoding='utf-8').read().split('\\n')\n",
    "    sentences=[]\n",
    "    for line in data:\n",
    "        pattern = r'\\[(.*?)\\]\\s+\\[(.*?)\\]\\s+(.+)'\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            name = match.group(1)  # 첫 번째 대괄호 안의 단어 추출\n",
    "            time = match.group(2)  # 두 번째 대괄호 안의 단어 추출\n",
    "            content = match.group(3)  # 대괄호 뒤의 내용 추출\n",
    "            # print(name, time, content)\n",
    "            temp=[name,time,content]\n",
    "            sentences.append(temp)    \n",
    "    return sentences\n",
    "\n",
    "#참여자 뽑기\n",
    "def get_user(katok):\n",
    "    # 중복을 제거하고 참여자 리스트 생성\n",
    "    user_names = list(set([i[0] for i in katok]))\n",
    "    return user_names\n",
    "\n",
    "#두명의 대화를 [[찬란카톡],[하영카톡]]형태로 얻는 함수\n",
    "def get_convers(user_names):\n",
    "    received_texts = [\n",
    "        [j[2] for j in get_from_txt('sample.txt') if i == j[0]]  ##\n",
    "        for i in user_names]\n",
    "    # 이모티콘, 사진, 샵검색 제거 \n",
    "    exclusion_list = ['사진', '이모티콘', '샵검색:']\n",
    "    convers_texts = [\n",
    "    [str(j) for j in i if all(exclusion not in j for exclusion in exclusion_list)]\n",
    "    for i in received_texts]\n",
    "    return convers_texts\n",
    "\n",
    "# 반복,띄어쓰기 정제(Cleaning) 함수\n",
    "def clean_korean_text(convers_texts):\n",
    "    convers_texts = re.sub(r'http\\S+', '', convers_texts)  #링크를 지운다\n",
    "    convers_texts = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣ\\s.!?~^;]', ' ', convers_texts)\n",
    "    convers_texts = repeat_normalize(convers_texts, num_repeats=2)\n",
    "    clean_texts = re.sub(r'\\s+', ' ', convers_texts).strip()\n",
    "    return clean_texts\n",
    "\n",
    "# 입력문장을 정제하고 공백대화를 제거함(https도 같이 없어짐)\n",
    "# [[찬란카톡],[하영카톡]]가 들어와서 정제하는 함수\n",
    "def get_clean(convers_texts):\n",
    "    temp = []\n",
    "    for convers_text in convers_texts:\n",
    "        temp.append([ clean_korean_text(i) for i in convers_text if clean_korean_text(i) !='' ]) ##\n",
    "    clean_texts = temp\n",
    "    del temp\n",
    "    return clean_texts\n",
    "\n",
    "#워드피스 토큰화 적용\n",
    "def tokenize(clean_texts):\n",
    "    temp = []\n",
    "    for clean_text in clean_texts: \n",
    "        clean_text = tokenizer(clean_text, truncation=True, padding='max_length', max_length=64,return_token_type_ids= False)\n",
    "        temp.append(clean_text)\n",
    "    tokenized_texts = temp\n",
    "    del temp\n",
    "    return tokenized_texts\n",
    "\n",
    "#모델예측 \n",
    "#함수에 입력되는 형태 [[찬란토큰화카톡],[하영토큰화카톡]] , user_names['김찬란','김하영']\n",
    "#                       강한부정   ~     강한긍정\n",
    "#출력되는 형태 : ['김찬란',[12, 23, 45, 65, 34],[강한부정들],[강한 긍정들]]\n",
    "#               , ['김하영',[12, 23, 45, 65, 34],[강한부정들],[강한 긍정들]]\n",
    "\n",
    "def get_prediction(tokenized_texts, user_names):\n",
    "    predicted = []\n",
    "    for order,tokenized_text in enumerate(tokenized_texts):\n",
    "        predictions=loaded_model.predict([\n",
    "            np.array(tokenized_text['input_ids']),\n",
    "            np.array(tokenized_text['attention_mask'])\n",
    "            ])\n",
    "        counts = [0, 0, 0, 0, 0]\n",
    "        index_of_strong_nega_texts= []\n",
    "        index_of_strong_posi_texts= []\n",
    "        cnt=0\n",
    "        for prediction in predictions[0]:\n",
    "            indices = np.around(tf.nn.softmax(prediction)).astype(int)\n",
    "            cnt+=1\n",
    "            #여기서 배열이 다 0이면 컨티뉴 해버린다.\n",
    "            if len(np.where(indices == 1)[0]) == 0:\n",
    "                continue\n",
    "            index = np.where(indices == 1)[0][0]\n",
    "            counts[index] += 1\n",
    "            #강부,강긍의 인덱스를저장한다.\n",
    "            if index == 0:\n",
    "                index_of_strong_nega_texts.append(cnt)\n",
    "            if index == 4:\n",
    "                index_of_strong_posi_texts.append(cnt)\n",
    "                \n",
    "        name=user_names[order]\n",
    "        #4개보다 작을경우 오류 방지\n",
    "        if len(index_of_strong_nega_texts)>=4:\n",
    "            strne = random.sample(index_of_strong_nega_texts, 3)\n",
    "        else :\n",
    "            strne = index_of_strong_nega_texts\n",
    "        #해당 인덱스 문장 가져오기\n",
    "        strne_texts = [ convers_texts[order][i] for i in strne]\n",
    "            \n",
    "        if len(index_of_strong_posi_texts)>=4:\n",
    "            strpo = random.sample(index_of_strong_posi_texts, 3)\n",
    "        else :\n",
    "            strpo = index_of_strong_posi_texts\n",
    "        strpo_texts = [ convers_texts[order][i] for i in strpo]\n",
    "\n",
    "        #['김찬란',[12, 23, 45, 65, 34],[강한부정들],[강한 긍정들]]\n",
    "        predicted.append([name, counts, strne_texts, strpo_texts])\n",
    "        \n",
    "    return predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 15s 2s/step\n",
      "6/6 [==============================] - 14s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['김찬란',\n",
       "  [37, 65, 4, 54, 16],\n",
       "  ['오타 미친다 자찬 개짝음', '라고할뻔', '기분이 너무안좋으니까 맜있는거라도 먹어야겠다'],\n",
       "  ['이뻣음', '글게', '잘빠졌다\\\\']],\n",
       " ['김하영',\n",
       "  [28, 60, 5, 67, 9],\n",
       "  ['뭐 업체도 업체 나름 이지만', '만두아니고', '힘들게 살'],\n",
       "  ['ㅋㅋㅋㅋ ', '잘자라 짤랑', '진짜 착한애를 갖다가']]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "katok =get_from_txt('./sample.txt')\n",
    "user_names = get_user(katok)\n",
    "convers_texts = get_convers(user_names)\n",
    "clean_texts = get_clean(convers_texts) #안에 clean_korean_text()포함\n",
    "tokenized_texts = tokenize(clean_texts)\n",
    "get_prediction(tokenized_texts,user_names)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
