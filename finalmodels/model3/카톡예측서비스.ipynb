{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 모델을 불러와서 카톡대화를 넣으면\\\n",
    "나타난 감정의 순위를 보여준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFAlbertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFAlbertForSequenceClassification.\n",
      "\n",
      "All the layers of TFAlbertForSequenceClassification were initialized from the model checkpoint at ./albert-kor-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카톡대화 불러와서 정제(정규화코드)하는 함수\n",
    "\n",
    "#참여자 뽑기\n",
    "def get_user(katok):\n",
    "    # 중복을 제거하고 참여자 리스트 생성\n",
    "    user_names = list(set([i[0] for i in katok]))\n",
    "    return user_names\n",
    "\n",
    "#두명의 대화를 [[찬란카톡],[하영카톡]]형태로 얻는 함수\n",
    "def get_convers(user_names,katok):\n",
    "    received_texts = [\n",
    "        [j[2] for j in katok if i == j[0]]  ##\n",
    "        for i in user_names]\n",
    "    # 이모티콘, 사진, 샵검색 제거 \n",
    "    exclusion_list = ['사진', '이모티콘', '샵검색:']\n",
    "    convers_texts = [\n",
    "    [str(j) for j in i if all(exclusion not in j for exclusion in exclusion_list)]\n",
    "    for i in received_texts]\n",
    "    return convers_texts\n",
    "\n",
    "# 반복,띄어쓰기 정제(Cleaning) 함수\n",
    "def clean_korean_text(convers_texts):\n",
    "    convers_texts = re.sub(r'http\\S+', '', convers_texts)  #링크를 지운다\n",
    "    convers_texts = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣ\\s.!?~^;]', ' ', convers_texts)\n",
    "    convers_texts = repeat_normalize(convers_texts, num_repeats=2)\n",
    "    clean_texts = re.sub(r'\\s+', ' ', convers_texts).strip()\n",
    "    return clean_texts\n",
    "\n",
    "# 입력문장을 정제하고 공백대화를 제거함(https도 같이 없어짐)\n",
    "# [[찬란카톡],[하영카톡]]가 들어와서 정제하는 함수\n",
    "def get_clean(convers_texts):\n",
    "    temp = []\n",
    "    for convers_text in convers_texts:\n",
    "        temp.append([ clean_korean_text(i) for i in convers_text if clean_korean_text(i) !='' ]) ##\n",
    "    clean_texts = temp\n",
    "    del temp\n",
    "    return clean_texts\n",
    "\n",
    "#워드피스 토큰화 적용\n",
    "def tokenize(clean_texts):\n",
    "    temp = []\n",
    "    for clean_text in clean_texts: \n",
    "        clean_text = tokenizer(clean_text, truncation=True, padding='max_length', max_length=64,return_token_type_ids= False)\n",
    "        temp.append(clean_text)\n",
    "    tokenized_texts = temp\n",
    "    del temp\n",
    "    return tokenized_texts\n",
    "\n",
    "#모델예측 \n",
    "#함수에 입력되는 형태 [[찬란토큰화카톡],[하영토큰화카톡]] , user_names['김찬란','김하영']\n",
    "#                       \n",
    "#출력되는 형태 : ['김찬란',[12, 23, 45, 12, 23, 45, 45],['공포', '놀람', '분노','슬픔', '중립', '행복', '혐오']]\n",
    "#               ['김찬란', [공포문장들],[슬픔문장들],,,,,]\n",
    "#              ,['김하영',[12, 23, 45, 12, 23, 45, 45],['공포', '놀람', '분노','슬픔', '중립', '행복', '혐오']]\n",
    "#               ['김하영', [공포문장들],[슬픔문장들],,,,,]\n",
    "\n",
    "def get_prediction(tokenized_texts, loaded_model, user_names):\n",
    "    predicted = []\n",
    "    for order,tokenized_text in enumerate(tokenized_texts):\n",
    "        predictions=loaded_model.predict([\n",
    "            np.array(tokenized_text['input_ids']),\n",
    "            np.array(tokenized_text['attention_mask'])\n",
    "            ])\n",
    "        counts = [0, 0, 0, 0, 0, 0, 0]\n",
    "        cnt=-1\n",
    "        for prediction in predictions[0]:\n",
    "            indices = np.around(tf.nn.softmax(prediction)).astype(int)\n",
    "            cnt+=1\n",
    "            #여기서 배열이 다 0이면 컨티뉴 해버린다.\n",
    "            if len(np.where(indices == 1)[0]) == 0:\n",
    "                continue\n",
    "            index = np.where(indices == 1)[0][0]\n",
    "            counts[index] += 1\n",
    "        name=user_names[order]\n",
    "        counts_emo_rank = sorted(counts, reverse=True)\n",
    "        #순위 정렬하기\n",
    "        emo_list = ['공포', '놀람', '분노', '슬픔', '중립', '행복', '혐오']\n",
    "        index_list = list(range(len(counts)))\n",
    "        emo_index_rank = sorted(index_list, key = lambda x: counts[x], reverse=True)\n",
    "        emo_rank = []\n",
    "        for i in emo_index_rank:\n",
    "            emo_rank.append(emo_list[i])\n",
    "        # [['김찬란',[12, 23, 45, 65, 34, 65, 34],[감정순위대로]], ['김하영',[12, 23, 45, 65, 34, 65, 34],[감정순위대로]]]\n",
    "        predicted.append([name, counts_emo_rank, emo_rank])\n",
    "    return predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 18s 3s/step\n",
      "6/6 [==============================] - 17s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['김찬란',\n",
       "  [39, 31, 26, 26, 16, 15, 15],\n",
       "  ['놀람', '행복', '슬픔', '중립', '혐오', '공포', '분노']],\n",
       " ['김하영',\n",
       "  [41, 29, 28, 22, 21, 16, 8],\n",
       "  ['행복', '놀람', '슬픔', '중립', '분노', '혐오', '공포']]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#      ['공포', '놀람', '분노', '슬픔', '중립', '행복', '혐오']\n",
    "#순서대로  0       1       2      3       4       5       6 \n",
    "def load_tokenizer(tokenizer_path):\n",
    "    with open(tokenizer_path, \"rb\") as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "    return tokenizer\n",
    "\n",
    "def predict_final(model_path, pklpath, katok):\n",
    "    loaded_model = TFAlbertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer= load_tokenizer(pklpath)\n",
    "    user_names = get_user(katok)\n",
    "    convers_texts = get_convers(user_names,katok)\n",
    "    clean_texts = get_clean(convers_texts) #안에 clean_korean_text()포함\n",
    "    tokenized_texts = tokenize(clean_texts)\n",
    "    predicted = get_prediction(tokenized_texts, loaded_model, user_names)\n",
    "    return predicted\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
