{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 모델을 불러와서 카톡대화를 넣으면\\\n",
    "[[유저1,부정,긍정],[유저2,부정,긍정]] 형태로 반환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from konlpy.tag import Okt\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 불러오기\n",
    "loaded_model= load_model('GRU_model_1.h5')\n",
    "\n",
    "#저장된 vectorizer 불러오기\n",
    "with open('./GRU_tokenizer.pkl', 'rb') as f:\n",
    "    loaded_tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카톡대화 불러와서 정제,(정규화코드)하는 함수\n",
    "def get_from_txt(txt):\n",
    "    data= open(txt,\"r\", encoding='utf-8').read().split('\\n')\n",
    "    sentences=[]\n",
    "    for line in data:\n",
    "        pattern = r'\\[(.*?)\\]\\s+\\[(.*?)\\]\\s+(.+)'\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            name = match.group(1)  # 첫 번째 대괄호 안의 단어 추출\n",
    "            time = match.group(2)  # 두 번째 대괄호 안의 단어 추출\n",
    "            content = match.group(3)  # 대괄호 뒤의 내용 추출\n",
    "            # print(name, time, content)\n",
    "            temp=[name,time,content]\n",
    "            sentences.append(temp)    \n",
    "    return sentences\n",
    "\n",
    "#참여자 뽑기\n",
    "def get_user(katok):\n",
    "    katok=get_from_txt('./sample.txt') #함수안에 함수 X\n",
    "    # 중복을 제거하고 참여자 리스트 생성\n",
    "    user_names = list(set([i[0] for i in katok]))\n",
    "    return user_names\n",
    "\n",
    "\n",
    "#두명의 대화를 [[찬란카톡],[하영카톡]]형태로 얻는 함수\n",
    "def get_convers(user_names):\n",
    "    received_texts = [\n",
    "        [j[2] for j in get_from_txt('sample.txt') if i == j[0]]  ##\n",
    "        for i in user_names]\n",
    "    # 이모티콘, 사진, 샵검색 제거 \n",
    "    exclusion_list = ['사진', '이모티콘', '샵검색:']\n",
    "    preclean_texts = [\n",
    "    [str(j) for j in i if all(exclusion not in j for exclusion in exclusion_list)]\n",
    "    for i in received_texts]\n",
    "    return preclean_texts\n",
    "\n",
    "# 텍스트 특수문자, 반복,띄어쓰기 정제(Cleaning) 함수\n",
    "def clean_korean_text(preclean_texts):\n",
    "    preclean_texts = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]', ' ', preclean_texts)\n",
    "    preclean_texts = repeat_normalize(preclean_texts, num_repeats=1)\n",
    "    clean_texts = re.sub(r'\\s+', ' ', preclean_texts).strip()\n",
    "    return clean_texts\n",
    "\n",
    "# 입력문장을 정제하고 공백대화를 제거함(https도 같이 없어짐)\n",
    "# [[찬란카톡],[하영카톡]]가 들어와서 정제하는 함수\n",
    "def get_clean(convers_texts):\n",
    "    temp = []\n",
    "    for i in convers_texts:\n",
    "        temp.append([ clean_korean_text(j) for j in i if clean_korean_text(j) !='' ]) ##\n",
    "    clean_texts = temp\n",
    "    del temp\n",
    "    return clean_texts\n",
    "\n",
    "#형태소 분리\n",
    "def get_morphs(clean_text):\n",
    "    okt= Okt()\n",
    "    temp1 =[]\n",
    "    for sentences in clean_text:\n",
    "        temp2 = []\n",
    "        for sent in sentences:\n",
    "            temp2.append(okt.morphs(sent))\n",
    "        temp1.append(temp2)\n",
    "    morphs_text =temp1\n",
    "    del temp1, temp2\n",
    "    return morphs_text\n",
    "\n",
    "#불용어 제거\n",
    "\n",
    "\n",
    "#벡터화\n",
    "def get_vectorizer(morphs_text):\n",
    "    temp=[]\n",
    "    for i in morphs_text:\n",
    "        test_sequences = loaded_tokenizer.texts_to_sequences(i)\n",
    "        #길이 제한은 모델학습할떄의 max_length\n",
    "        paded_sequences = pad_sequences(test_sequences,padding='post',maxlen=23) \n",
    "        temp.append(paded_sequences)  \n",
    "    vetorized_text=temp\n",
    "    del temp\n",
    "    return vetorized_text\n",
    "\n",
    "#모델예측 [['김찬란',123,34],['김하영',234,547]]  [[유저1,부정,긍정],[유저2,부정,긍정]]\n",
    "def get_predict(vetorized_text):\n",
    "    predicted =[]\n",
    "    for order,sequence in enumerate(vetorized_text):\n",
    "        prediction = loaded_model.predict(sequence)  \n",
    "        cnt0=0; cnt1=0 \n",
    "        for i in prediction.squeeze().tolist():\n",
    "            if i < 0.5:  #부정이면\n",
    "                cnt0+=1\n",
    "            else :    #긍정이면 \n",
    "                cnt1+=1\n",
    "        global user_names    #함수의 인수로 받아오게끔 변경\n",
    "        name=user_names[order] \n",
    "        predicted.append([name,cnt0,cnt1])\n",
    "    return predicted\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 17ms/step\n",
      "6/6 [==============================] - 0s 19ms/step\n",
      "[['김찬란', 132, 45], ['김하영', 126, 47]]\n"
     ]
    }
   ],
   "source": [
    "katok=get_from_txt('./sample.txt')\n",
    "user_names=get_user(katok)\n",
    "convers_texts = get_convers(user_names)\n",
    "clean_text = get_clean(convers_texts)\n",
    "morphs_text = get_morphs(clean_text)\n",
    "vetorized_text =get_vectorizer(morphs_text)\n",
    "predicted= get_predict(vetorized_text)\n",
    "print(predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
