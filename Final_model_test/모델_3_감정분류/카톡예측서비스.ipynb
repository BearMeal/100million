{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 모델을 불러와서 카톡대화를 넣으면\\\n",
    "나타난 감정의 순위를 보여준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFAlbertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFAlbertForSequenceClassification.\n",
      "\n",
      "All the layers of TFAlbertForSequenceClassification were initialized from the model checkpoint at ./albert-kor-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 저장된 모델 불러오기\n",
    "loaded_model = TFAlbertForSequenceClassification.from_pretrained('./albert-kor-base')\n",
    "\n",
    "    \n",
    "# 피클 파일에서 BertTokenizerFast 객체 로드\n",
    "with open('./albert_tokenizer.pkl', \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카톡대화 불러와서 정제(정규화코드)하는 함수\n",
    "def get_from_txt(txt):\n",
    "    data= open(txt,\"r\", encoding='utf-8').read().split('\\n')\n",
    "    sentences=[]\n",
    "    for line in data:\n",
    "        pattern = r'\\[(.*?)\\]\\s+\\[(.*?)\\]\\s+(.+)'\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            name = match.group(1)  # 첫 번째 대괄호 안의 단어 추출\n",
    "            time = match.group(2)  # 두 번째 대괄호 안의 단어 추출\n",
    "            content = match.group(3)  # 대괄호 뒤의 내용 추출\n",
    "            # print(name, time, content)\n",
    "            temp=[name,time,content]\n",
    "            sentences.append(temp)    \n",
    "    return sentences\n",
    "\n",
    "#참여자 뽑기\n",
    "def get_user(katok):\n",
    "    # 중복을 제거하고 참여자 리스트 생성\n",
    "    user_names = list(set([i[0] for i in katok]))\n",
    "    return user_names\n",
    "\n",
    "#두명의 대화를 [[찬란카톡],[하영카톡]]형태로 얻는 함수\n",
    "def get_convers(user_names):\n",
    "    received_texts = [\n",
    "        [j[2] for j in get_from_txt('sample.txt') if i == j[0]]  ##\n",
    "        for i in user_names]\n",
    "    # 이모티콘, 사진, 샵검색 제거 \n",
    "    exclusion_list = ['사진', '이모티콘', '샵검색:']\n",
    "    convers_texts = [\n",
    "    [str(j) for j in i if all(exclusion not in j for exclusion in exclusion_list)]\n",
    "    for i in received_texts]\n",
    "    return convers_texts\n",
    "\n",
    "# 반복,띄어쓰기 정제(Cleaning) 함수\n",
    "def clean_korean_text(convers_texts):\n",
    "    convers_texts = re.sub(r'http\\S+', '', convers_texts)  #링크를 지운다\n",
    "    convers_texts = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣ\\s.!?~^;]', ' ', convers_texts)\n",
    "    convers_texts = repeat_normalize(convers_texts, num_repeats=2)\n",
    "    clean_texts = re.sub(r'\\s+', ' ', convers_texts).strip()\n",
    "    return clean_texts\n",
    "\n",
    "# 입력문장을 정제하고 공백대화를 제거함(https도 같이 없어짐)\n",
    "# [[찬란카톡],[하영카톡]]가 들어와서 정제하는 함수\n",
    "def get_clean(convers_texts):\n",
    "    temp = []\n",
    "    for convers_text in convers_texts:\n",
    "        temp.append([ clean_korean_text(i) for i in convers_text if clean_korean_text(i) !='' ]) ##\n",
    "    clean_texts = temp\n",
    "    del temp\n",
    "    return clean_texts\n",
    "\n",
    "#워드피스 토큰화 적용\n",
    "def tokenize(clean_texts):\n",
    "    temp = []\n",
    "    for clean_text in clean_texts: \n",
    "        clean_text = tokenizer(clean_text, truncation=True, padding='max_length', max_length=64,return_token_type_ids= False)\n",
    "        temp.append(clean_text)\n",
    "    tokenized_texts = temp\n",
    "    del temp\n",
    "    return tokenized_texts\n",
    "\n",
    "#모델예측 \n",
    "#함수에 입력되는 형태 [[찬란토큰화카톡],[하영토큰화카톡]] , user_names['김찬란','김하영']\n",
    "#                       \n",
    "#출력되는 형태 : ['김찬란',[12, 23, 45, 12, 23, 45, 45],['공포', '놀람', '분노','슬픔', '중립', '행복', '혐오']]\n",
    "#               ['김찬란', [공포문장들],[슬픔문장들],,,,,]\n",
    "#              ,['김하영',[12, 23, 45, 12, 23, 45, 45],['공포', '놀람', '분노','슬픔', '중립', '행복', '혐오']]\n",
    "#               ['김하영', [공포문장들],[슬픔문장들],,,,,]\n",
    "\n",
    "def get_prediction(tokenized_texts, loaded_model, user_names):\n",
    "    predicted = []\n",
    "    for order,tokenized_text in enumerate(tokenized_texts):\n",
    "        predictions=loaded_model.predict([\n",
    "            np.array(tokenized_text['input_ids']),\n",
    "            np.array(tokenized_text['attention_mask'])\n",
    "            ])\n",
    "        counts = [0, 0, 0, 0, 0, 0, 0]\n",
    "        cnt=-1\n",
    "        for prediction in predictions[0]:\n",
    "            indices = np.around(tf.nn.softmax(prediction)).astype(int)\n",
    "            cnt+=1\n",
    "            #여기서 배열이 다 0이면 컨티뉴 해버린다.\n",
    "            if len(np.where(indices == 1)[0]) == 0:\n",
    "                continue\n",
    "            index = np.where(indices == 1)[0][0]\n",
    "            counts[index] += 1\n",
    "        name=user_names[order]\n",
    "        counts_emo_rank = sorted(counts, reverse=True)\n",
    "        #순위 정렬하기\n",
    "        emo_list = ['공포', '놀람', '분노', '슬픔', '중립', '행복', '혐오']\n",
    "        index_list = list(range(len(counts)))\n",
    "        emo_index_rank = sorted(index_list, key = lambda x: counts[x], reverse=True)\n",
    "        emo_rank = []\n",
    "        for i in emo_index_rank:\n",
    "            emo_rank.append(emo_list[i])\n",
    "        # [['김찬란',[12, 23, 45, 65, 34, 65, 34],[감정순위대로]], ['김하영',[12, 23, 45, 65, 34, 65, 34],[감정순위대로]]]\n",
    "        predicted.append([name, counts_emo_rank, emo_rank])\n",
    "    return predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 18s 3s/step\n",
      "6/6 [==============================] - 17s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['김찬란',\n",
       "  [39, 31, 26, 26, 16, 15, 15],\n",
       "  ['놀람', '행복', '슬픔', '중립', '혐오', '공포', '분노']],\n",
       " ['김하영',\n",
       "  [41, 29, 28, 22, 21, 16, 8],\n",
       "  ['행복', '놀람', '슬픔', '중립', '분노', '혐오', '공포']]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#      ['공포', '놀람', '분노', '슬픔', '중립', '행복', '혐오']\n",
    "#순서대로  0       1       2      3       4       5       6 \n",
    "\n",
    "katok =get_from_txt('./sample.txt')\n",
    "user_names = get_user(katok)\n",
    "convers_texts = get_convers(user_names)\n",
    "clean_texts = get_clean(convers_texts) #안에 clean_korean_text()포함\n",
    "tokenized_texts = tokenize(clean_texts)\n",
    "get_prediction(tokenized_texts, loaded_model, user_names)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 817ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['김찬란', [6, 2, 1, 0, 0, 0, 0], ['분노', '행복', '놀람', '공포', '슬픔', '중립', '혐오']],\n",
       " ['김하영', [4, 3, 1, 0, 0, 0, 0], ['분노', '놀람', '혐오', '공포', '슬픔', '중립', '행복']]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "user_names = ['김찬란', '김하영']\n",
    "convers_texts = [['너는 어쩔려고 인생 그렇게 사냐?','욕하지 마라 시발롬아 뒤지고 싶나 손에 든거부터 내려놓고 해달라','그래 돼지년아'\n",
    "  ,'씹돼지새끼야','시뱅아','개잡년아','디질래','야이 새끼야 니얼굴에 공부도 못하면 답없다','ㅋㅋㅋㅋㅋ'],\n",
    " ['남이사 인생 어떻게 살든말든 니가 뭔상관인데 개새끼야','너 근데 왜 중간중간에 자꾸 돼지 넣어?','병신이냐?','그게 자극이냐?',\n",
    "  '사람놀리는거지 뱅쉰아?','야이 애미출타한 씨팔년아','못한다고 씨발 좆같은년아','됐지?']]\n",
    "clean_texts = get_clean(convers_texts)\n",
    "tokenized_texts = tokenize(clean_texts)\n",
    "\n",
    "predicted_1 = []\n",
    "# predicted_2 = []\n",
    "for order,tokenized_text in enumerate(tokenized_texts):\n",
    "    predictions=loaded_model.predict([\n",
    "        np.array(tokenized_text['input_ids']),\n",
    "        np.array(tokenized_text['attention_mask'])\n",
    "        ])\n",
    "    counts = [0, 0, 0, 0, 0, 0, 0]\n",
    "    index_of_fear_texts= []\n",
    "    index_of_surprised_texts= []\n",
    "    index_of_anger_texts= []\n",
    "    # index_of_sadness_texts= []\n",
    "    # index_of_neutral_texts= []\n",
    "    # index_of_happy_texts= []\n",
    "    # index_of_aversion_texts= []  #혐오\n",
    "    cnt=-1\n",
    "    for prediction in predictions[0]:\n",
    "        indices = np.around(tf.nn.softmax(prediction)).astype(int)\n",
    "        cnt+=1\n",
    "        #여기서 배열이 다 0이면 컨티뉴 해버린다.\n",
    "        if len(np.where(indices == 1)[0]) == 0:\n",
    "            continue\n",
    "        index = np.where(indices == 1)[0][0]\n",
    "        counts[index] += 1\n",
    "        #강부,강긍의 인덱스를저장한다.\n",
    "        if index == 0:\n",
    "            index_of_fear_texts.append(cnt)\n",
    "        if index == 1:\n",
    "            index_of_surprised_texts.append(cnt)\n",
    "        if index == 2:\n",
    "            index_of_anger_texts.append(cnt)\n",
    "\n",
    "            \n",
    "    name=user_names[order]\n",
    "    fear_texts = [ convers_texts[order][i] for i in index_of_fear_texts]\n",
    "    surprised_texts = [ convers_texts[order][i] for i in index_of_surprised_texts]\n",
    "    anger_texts = [ convers_texts[order][i] for i in index_of_anger_texts]\n",
    "    \n",
    "    #순위 정렬하기\n",
    "    emo_list = ['공포', '놀람', '분노', '슬픔', '중립', '행복', '혐오']\n",
    "    index_list = list(range(len(counts)))\n",
    "    emo_index_rank = sorted(index_list, key = lambda x: counts[x], reverse=True)\n",
    "    emo_rank = []\n",
    "    for i in emo_index_rank:\n",
    "        emo_rank.append(emo_list[i])\n",
    "    counts_emo_rank = sorted(counts, reverse=True)\n",
    "    \n",
    "\n",
    "    #['김찬란',[12, 23, 45, 65, 34, 65, 34],[감정순위대로]]\n",
    "    predicted_1.append([name, counts_emo_rank, emo_rank])\n",
    "    # predicted_2.append([name, fear_texts, surprised_texts, anger_texts])\n",
    "    \n",
    "predicted_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
